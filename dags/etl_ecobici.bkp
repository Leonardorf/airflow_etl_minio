from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.docker.operators.docker import DockerOperator
from docker.types import Mount
from datetime import datetime
import os
import subprocess

default_args = {"owner": "airflow", "start_date": datetime(2024, 1, 1)}

with DAG("etl_ecobici",
         default_args=default_args,
         schedule_interval=None,
         catchup=False) as dag:

    # Tarea 1: Descargar CSV crudo
    def descargar_csv():
        import requests
        os.makedirs("data/raw", exist_ok=True)
        url = "https://cdn.buenosaires.gob.ar/datosabiertos/datasets/transporte-y-obras-publicas/bicicletas-publicas/usuarios_ecobici_2024.csv"
        r = requests.get(url)
        with open("data/raw/usuarios_ecobici_2024.csv", "wb") as f:
            f.write(r.content)

    # Tarea 3: Subir a MinIO
    def subir_a_minio():
        subprocess.run(["python", "dags/scripts/subir_minio.py"], check=True)

    # Operator 1
    descargar = PythonOperator(
        task_id="descargar_csv",
        python_callable=descargar_csv
    )

    # Operator 2: DockerOperator ejecutando spark-submit en contenedor spark
    procesar = DockerOperator(
        task_id="procesar_datos",
        image="bitnami/spark",
        container_name="spark",
        command="spark-submit /app/dags/scripts/procesar_ecobici.py",
        mounts=[
            Mount(
                source="C:/Users/Leonardo Villegas/Documents/airflow_etl_minio",
                target="/app",
                type="bind"
            )
        ],
        auto_remove=True,
        network_mode="bridge",
        mount_tmp_dir=False,
        tty=True
    )

    # Operator 3
    subir = PythonOperator(
        task_id="subir_minio",
        python_callable=subir_a_minio
    )

    # Dependencias
    descargar >> procesar >> subir
