# ğŸ› ï¸ Proyecto ETL con Airflow, PySpark y MinIO

Este proyecto implementa un pipeline ETL utilizando Apache Airflow para orquestar tareas que descargan datos abiertos, los procesan con PySpark y los almacenan en MinIO, una soluciÃ³n local que emula un bucket S3.

ğŸ’¡ Fue desarrollado y probado en **Windows** usando **Docker Desktop**.

---

## ğŸ“Š Fuente de datos

Este proyecto utiliza un dataset de acceso pÃºblico de la Ciudad AutÃ³noma de Buenos Aires (CABA):

- **Usuarios del sistema de bicicletas pÃºblicas EcoBici**  
  Fuente: [datos abiertos CABA](https://data.buenosaires.gob.ar/dataset/bicicletas-publicas)  
  CSV directo: [usuarios_ecobici_2024.csv](https://cdn.buenosaires.gob.ar/datosabiertos/datasets/transporte-y-obras-publicas/bicicletas-publicas/usuarios_ecobici_2024.csv)

ğŸ” **Uso sugerido**: anÃ¡lisis de patrones de uso por gÃ©nero, tipo de usuario (mensual / ocasional), comportamiento de movilidad sustentable, distribuciÃ³n temporal, etc.

---

## ğŸ” Objetivo

Simular un entorno moderno de ingenierÃ­a de datos con herramientas de orquestaciÃ³n (Airflow), procesamiento distribuido (Spark) y almacenamiento tipo S3 (MinIO). Ideal para demostrar habilidades en proyectos ETL dentro de un portfolio de ciencia de datos.

---

## ğŸ§° TecnologÃ­as utilizadas

- ğŸª‚ **Apache Airflow**: OrquestaciÃ³n de workflows
- âš¡ **Apache Spark / PySpark**: Procesamiento de datos
- ğŸ“¦ **MinIO**: Almacenamiento local estilo S3
- ğŸ³ **Docker + Docker Compose**: Contenedores para el entorno completo
- ğŸ **Python**
- ğŸ“Š (Opcional) Jupyter Notebook para anÃ¡lisis posterior

---

## ğŸ“ Estructura del proyecto

```bash
airflow_etl_minio/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ etl_minio_dag.py              # DAG principal
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ descargar_datos.py        # Descarga datasets
â”‚       â”œâ”€â”€ procesar_spark.py         # Procesamiento PySpark
â”‚       â””â”€â”€ subir_a_minio.py          # Carga a MinIO vÃ­a boto3
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml            # Servicios: Airflow, Spark, MinIO
â”‚   â”œâ”€â”€ Dockerfile.spark              # Imagen personalizada para Spark
â”‚   â””â”€â”€ requirements-airflow.txt      # Requisitos de Airflow
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploracion.ipynb             # ExploraciÃ³n y visualizaciÃ³n
â”œâ”€â”€ data/                             # (Opcional) Datos locales
â”œâ”€â”€ .env.example                      # Variables de entorno
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸš€ CÃ³mo ejecutar el proyecto

### âœ… Requisitos previos

- [Docker Desktop](https://www.docker.com/products/docker-desktop/) instalado en Windows
- Git

### ğŸ§ª Instrucciones

1. **ClonÃ¡ el repositorio**
   ```bash
   git clone https://github.com/Leonardorf/airflow_etl_minio.git
   cd airflow_etl_minio
   ```

2. **ConfigurÃ¡ las variables de entorno**
   ```bash
   cp .env.example .env
   ```

3. **LevantÃ¡ los servicios**
   ```bash
   docker-compose -f docker/docker-compose.yml up --build
   ```

4. **AccedÃ© a las interfaces**
   - ğŸŒ¬ï¸ Airflow: [http://localhost:8080](http://localhost:8080)
   - ğŸª£ MinIO: [http://localhost:9001](http://localhost:9001)
     - Usuario: `minio`
     - ContraseÃ±a: `minio123`

---

## âš™ï¸ Detalles del DAG

El DAG `etl_minio_dag.py` define el siguiente flujo de trabajo:

1. **Descargar datos** desde una fuente pÃºblica (ej. movilidad urbana)
2. **Procesar datos** con PySpark (limpieza, transformaciÃ³n, agregados)
3. **Subir resultados** a un bucket en MinIO (como si fuera Amazon S3)

---

## ğŸ““ AnÃ¡lisis y visualizaciÃ³n

PodÃ©s explorar los datos procesados usando el notebook ubicado en `notebooks/exploracion.ipynb`, conectÃ¡ndote a MinIO o leyendo los archivos locales descargados.

---

## ğŸ“‚ .env.example

```dotenv
# MinIO
MINIO_ACCESS_KEY=minio
MINIO_SECRET_KEY=minio123
S3_ENDPOINT=http://minio:9000
```

---

## ğŸ›‘ .gitignore recomendado

```gitignore
__pycache__/
*.pyc
.env
data/
tmp/
logs/
```

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ disponible bajo la licencia MIT. Ver archivo `LICENSE` para mÃ¡s detalles.

---

## ğŸ‘¤ Autor

Desarrollado por **Leonardorf** como parte de su portfolio de ciencia de datos.  
GitHub: [github.com/Leonardorf](https://github.com/Leonardorf)

---

## ğŸ Estado del proyecto

âœ… Funcional y probado localmente en Windows con Docker Desktop.  
ğŸ› ï¸ Puede expandirse para ejecutar sobre S3 real o incluir visualizaciÃ³n automÃ¡tica.
